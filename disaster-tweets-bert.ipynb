{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whs0n7X_rSVH",
        "outputId": "f1b35813-d2eb-47ab-cca4-6ca0a9d152b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aym7Pyu6rONF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/projects/kaggle-disaster/train.csv', index_col = 0)\n",
        "test = pd.read_csv('/content/drive/MyDrive/projects/kaggle-disaster/test.csv', index_col = 0)\n",
        "sub = pd.read_csv('/content/drive/MyDrive/projects/kaggle-disaster/sample_submission.csv')"
      ],
      "metadata": {
        "id": "2wjnNp-0rajp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ijjqab0Rdk",
        "outputId": "5be57cdc-3e65-4b64-d84b-1bc52834ba50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7613, 4)\n",
            "(3263, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "-eaaj6onr7tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "best score : \n",
        "- col_concat 적용\n",
        "- rm_stopwords 미적용\n",
        "- rp_pattern 미적용\n",
        "- 중복 제거 적용"
      ],
      "metadata": {
        "id": "7wHP5O3lopme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## keyword & location"
      ],
      "metadata": {
        "id": "EoBFGGMzuM1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def col_concat(df):\n",
        "    df.keyword.fillna('', inplace=True)\n",
        "    df[\"text\"] = df[\"keyword\"].astype(str) + ' ' + df[\"text\"]\n",
        "\n",
        "    # df['location'] = df.location.where(~df.location.notna(), 'LOCATION')\n",
        "    # df.location.fillna('', inplace=True)\n",
        "    # df[\"text\"] = df[\"location\"].astype(str) + ' ' + df[\"text\"]"
      ],
      "metadata": {
        "id": "JfcovO3V_clG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_concat(train)\n",
        "col_concat(test)"
      ],
      "metadata": {
        "id": "co648I4XAHLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lowercase + remove stopwords"
      ],
      "metadata": {
        "id": "aNP2htu7td5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cuc__kEr3En",
        "outputId": "90d79133-ab01-435b-8c05-579075a7a46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_stopwords(df, stopwords):\n",
        "    df['text'] = df['text'].str.lower()\n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
      ],
      "metadata": {
        "id": "hRlsD1b6sde5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm_stopwords(train, stopwords)\n",
        "rm_stopwords(test, stopwords)"
      ],
      "metadata": {
        "id": "4ZDPnOBjAk5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## masking/replacing using regex"
      ],
      "metadata": {
        "id": "SD47jjo33APD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rp_pattern(df):\n",
        "    url_ptrn = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
        "    time_ptrn = '[0-9]{2}:[0-9]{2}(:[0-9]{2})*'\n",
        "    web_ptrn = '&[a-zA-Z]*;'\n",
        "\n",
        "    df.text.replace(to_replace=url_ptrn, value='URL', regex=True, inplace=True)\n",
        "    df.text.replace(to_replace=time_ptrn, value='TIME', regex=True, inplace=True)\n",
        "    df.text.replace(to_replace=web_ptrn, value=' ', regex=True, inplace=True)\n",
        "\n",
        "    df.replace('%', ' ', inplace=True)\n",
        "    df.replace('\\n', ' ', inplace=True)"
      ],
      "metadata": {
        "id": "o9KLW4XXDivq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rp_pattern(train)\n",
        "rp_pattern(test)"
      ],
      "metadata": {
        "id": "mmgAl05eDsFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove duplicates / misslabelled"
      ],
      "metadata": {
        "id": "cuQPZOEitjcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop_duplicates(subset=['text'], keep=False)"
      ],
      "metadata": {
        "id": "aZ3YL4fQ0dJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split"
      ],
      "metadata": {
        "id": "9DceSTc7ILJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stratify 옵션, test size 작게 (train 데이터 크기가 작은 관계로)"
      ],
      "metadata": {
        "id": "xX8PEddjo-Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#stratify\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train['text'], train['target'], stratify=train['target'], test_size=0.1)"
      ],
      "metadata": {
        "id": "nPWJZ23bIK4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification using BERT"
      ],
      "metadata": {
        "id": "oSkm4zNyr3-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-for-tf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg_cxKXerjD-",
        "outputId": "6c0a566b-0c46-4b2f-9251-df7321c8df41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 147 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.0)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=586cdb65c8357e3d2e2ed30e497b191a3f53031118fef6ba5309d281bf2a8979\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=07d6c552543e78753198a95529893a2ad1176c5cc257d8a964455c0f89f6031b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=97617c8998441fe9ae3753a592cef5fbb53f48f4acbbb940dae04cd4b6b0f1a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert import bert_tokenization\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "h1hcio7irmO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=True)"
      ],
      "metadata": {
        "id": "t-Wl_bu_rpOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 구조 좀더 단순하게 조정"
      ],
      "metadata": {
        "id": "cxK70GjSpNN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\n",
        "    # net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
        "    # net = tf.keras.layers.Dropout(0.2)(net)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
        "\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "d8RMWWcFrre3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_len 을 120에서 80으로 조정"
      ],
      "metadata": {
        "id": "jbCEZSh1pKh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 80\n",
        "x_train = bert_encode(x_train.values, tokenizer, max_len=max_len)\n",
        "x_valid = bert_encode(x_valid.values, tokenizer, max_len=max_len)\n",
        "train_labels = y_train.values\n",
        "valid_labels = y_valid.values"
      ],
      "metadata": {
        "id": "Fj2avPoCIX-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BdTCviErv6C",
        "outputId": "edc2bc7d-c134-4ed1-e24d-88f2ffde16f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_2 (KerasLayer)     [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
            "                                 (None, 80, 768)]                 'input_mask[0][0]',             \n",
            "                                                                  'segment_ids[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['keras_layer_2[0][1]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          98432       ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1)            129         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,580,802\n",
            "Trainable params: 109,580,801\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/projects/kaggle-disaster/model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_history = model.fit(\n",
        "    x_train, y_train, \n",
        "    validation_data=(x_valid,y_valid),\n",
        "    epochs=30,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SL3S3gy2NnV",
        "outputId": "ef3b4773-6d93-4b9d-fd1a-cb1d572ec025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "421/421 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.8039\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85829, saving model to /content/drive/MyDrive/projects/kaggle-disaster/model.h5\n",
            "421/421 [==============================] - 147s 320ms/step - loss: 0.4421 - accuracy: 0.8039 - val_loss: 0.3583 - val_accuracy: 0.8583\n",
            "Epoch 2/30\n",
            "421/421 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8608\n",
            "Epoch 2: val_accuracy did not improve from 0.85829\n",
            "421/421 [==============================] - 120s 285ms/step - loss: 0.3402 - accuracy: 0.8608 - val_loss: 0.3662 - val_accuracy: 0.8436\n",
            "Epoch 3/30\n",
            "421/421 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.8987\n",
            "Epoch 3: val_accuracy did not improve from 0.85829\n",
            "421/421 [==============================] - 121s 287ms/step - loss: 0.2677 - accuracy: 0.8987 - val_loss: 0.4056 - val_accuracy: 0.8436\n",
            "Epoch 4/30\n",
            "421/421 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9269\n",
            "Epoch 4: val_accuracy did not improve from 0.85829\n",
            "421/421 [==============================] - 121s 288ms/step - loss: 0.1978 - accuracy: 0.9269 - val_loss: 0.4535 - val_accuracy: 0.8436\n",
            "Epoch 5/30\n",
            " 89/421 [=====>........................] - ETA: 1:33 - loss: 0.1366 - accuracy: 0.9494"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "model.load_weights('/content/drive/MyDrive/projects/kaggle-disaster/model.h5')\n",
        "test_input = bert_encode(test.text.values, tokenizer, max_len=max_len)\n",
        "test_pred = model.predict(test_input)\n",
        "sub['target'] = test_pred.round().astype(int)\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "sub.to_csv(f'/content/drive/MyDrive/projects/kaggle-disaster/submission-{timestr}.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvkKbwuSr1zY",
        "outputId": "820fd3f5-2bbb-47b3-bc74-371519071781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.4 s, sys: 355 ms, total: 20.8 s\n",
            "Wall time: 32.6 s\n"
          ]
        }
      ]
    }
  ]
}